<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shitian Zhao</title>

    <meta name="author" content="Shitian Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shitian Zhao
                </p>
                <p>I am a senior student at the  [Department of Electronic Engineering, East China Normal University](http://www.cee.ecnu.edu.cn/). And I was an intern in [CCVL@Johns Hopkins University](https://ccvl.jhu.edu/), supervised by [Bloomberg Distinguished Professor Alan Yuille](https://www.cs.jhu.edu/~ayuille/) and [Zhuowan Li](https://lizw14.github.io/).
                </p>
                <p style="text-align:center">
                  <a href="mailto:10202140413@stu.ecnu.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="data/stzhao_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=84t2PkoAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/zst96687522">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zhaoshitian/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/zst.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/JonBarron.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My interest includes Multi-modal Language Model and Diffusion Model.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <!-- <p>
                  <li><span style="background-color: #FFFF00"> New </span><font color="orange"><strong>On the job market</strong></font>: I am currently on the job market. I am interested in both industry and academic positions. Donâ€™t hesitate to email me if there is a potential fit.</p>
              <ul> -->
                <li><strong>[June 2023]</strong> I will attend CVPR 2023 in person at Vancouver. Let me know if you want to talk with me!</li>
                <li><strong>[May 2023]</strong> Started as as applied scentist intern at Amazon AWS.</li>
                <li><strong>[May 2023]</strong> Invited talk at <a href="https://cocosci.mit.edu/josh"> the Computational Cognitive Science Lab </a> at <a href="https://www.mit.edu">MIT</a>.</li>
                <li><strong>[February 2023]</strong> Super-CLEVR is accepted by CVPR 2023 as Highlight.</li>
              </ul>
                <p style="text-align:right;font-size:small;">
                  Last updated: 2023/10/29.
                </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- <tr bgcolor="#ffffd0"> -->

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2023_3D_superclevr.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2310.17914.pdf">
          <papertitle>3D-Aware Visual Question Answering
            about Parts, Poses and Occlusions</papertitle>
        </a>
        <br>
        <a href="https://xingruiwang.github.io">Xingrui Wang</a>,
        <a href="https://wufeim.github.io">Wufei Ma</a>,
        <strong>Zhuowan Li</strong>,
        <a href="https://adamkortylewski.com">Adam Kortylewski</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>NeurIPS</em>, 2023
        <br>       
        <a href="https://arxiv.org/pdf/2310.17914.pdf">arXiv</a> /
        <a href="https://github.com/XingruiWang/3D-Aware-VQA">code and dataset</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2023_superclevr.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://lizw14.github.io/project/2023_SuperCLEVR">
          <papertitle>Super-CLEVR: A Virtual Benchmark to Diagnose Domain Robustness in Visual Reasoning</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://xingruiwang.github.io">Xingrui Wang</a>,
        <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
        <a href="https://adamkortylewski.com">Adam Kortylewski</a>,
        <a href="https://wufeim.github.io">Wufei Ma</a>,
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>CVPR (Highlight, top 2.5%)</em>, 2023
        <br>
        <a href="https://lizw14.github.io/project/2023_SuperCLEVR">project page</a> /
        <a href="https://arxiv.org/pdf/2212.00259.pdf">arXiv</a> /
        <a href="https://github.com/Lizw14/Super-CLEVR">code and dataset</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2022_visual_probing.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2212.00281.pdf">
          <papertitle>Localization vs. Semantics: How Can Language Benefit Visual Representation Learning?</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://cihangxie.github.io/">Cihang Xie</a>,
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>Under submission</em>, 2023
        <br>
        <a href="https://arxiv.org/pdf/2212.00281.pdf">arXiv</a> /
        <a href="https://github.com/Lizw14/visual_probing">code</a> (to be released)
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2022_vl_commonsense.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2205.01850.pdf">
          <papertitle>Visual Commonsense in Pretrained Unimodal and Multimodal Models</papertitle>
        </a>
        <br>
        <a href="https://chenyu-zhang.appspot.com">Chenyu Zhang</a>
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <strong>Zhuowan Li*</strong>,
        <a href="https://esteng.github.io/">Elias Stengel-Eskin*</a>,
        <br>
        <em>NAACL (Oral)</em>, 2022
        <br>
        <a href="https://arxiv.org/pdf/2205.01850.pdf">arXiv</a> /
        <a href="https://github. com/ChenyuHeidiZhang/VL-commonsense">code and dataset</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2022_swapmix.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2204.02285.pdf">
          <papertitle>SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering</papertitle>
        </a>
        <br>
        <a href="https://vipulgupta1011.github.io">Vipul Gupta</a>,
        <strong>Zhuowan Li</strong>,
        <a href="https://adamkortylewski.com">Adam Kortylewski</a>,
        <a href="https://angelicaz.github.io">Chenyu Zhang</a>,
        <a href="https://yingwei.li">Yingwei Li</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://arxiv.org/pdf/2204.02285.pdf">arXiv</a> /
        <a href="https://github.com/vipulgupta1011/swapmix">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2021_calico.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2110.00519.pdf">
          <papertitle>Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://esteng.github.io/">Elias Stengel-Eskin</a>,
        <a href="https://scholar.google.com/citations?user=lU3wroMAAAAJ&hl=zh-CN">Yixiao Zhang</a>,
        <a href="https://cihangxie.github.io/">Cihang Xie</a>,
        <a href="https://scholar.google.com/citations?user=ehs5ImcAAAAJ&hl=en">Quan Tran</a>,
        <a href="https://www.cs.jhu.edu/~vandurme/index.html">Benjamin Van Durme</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>ICCV</em>, 2021
        <br>
        <a href="https://arxiv.org/pdf/2110.00519.pdf">arXiv</a> /
        <a href="https://github.com/Lizw14/CaliCO.git">code</a>
        <p></p>
      </td>
    </tr>

    

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2020_groupcap.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://lizw14.github.io/project/groupcap">
          <papertitle>Context-Aware Group Captioning via Self-Attention and Contrastive Features</papertitle>
        </a>
        <br>
        <strong>Zhuowan Li</strong>,
        <a href="https://scholar.google.com/citations?user=ehs5ImcAAAAJ&hl=en">Quan Tran</a>,
        <a href="https://mai-t-long.com">Long Mai</a>,
        <a href="https://scholar.google.com/citations?user=R0bnqaAAAAAJ&hl=en">Zhe Lin</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>
        <br>
        <em>CVPR</em>, 2020
        <br>
        <a href="https://arxiv.org/pdf/2004.03708.pdf">arXiv</a> /
        <a href="https://lizw14.github.io/project/groupcap">project page</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/2018_fdgan.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/1810.02936.pdf">
          <papertitle>FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</papertitle>
        </a>
        <br>
        <a href="https://geyixiao.com">Yixiao Ge*</a>,
        <strong>Zhuowan Li*</strong>,
        <a href="https://scholar.google.com/citations?user=oGM5N1kAAAAJ">Haiyu Zhao</a>,
        <a href="https://gjyin91.github.io/">Guojun Yin</a>,
        <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ">Shuai Yi</a>,
        <a href="https://www.ee.cuhk.edu.hk/~xgwang/">Xiaogang Wang</a>,
        <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
        <br>
        <em>NeurIPS</em>, 2018
        <br>
        <a href="https://arxiv.org/pdf/1810.02936.pdf">arXiv</a> /
        <a href="https://geyixiao.com/projects/fdgan.html">project page</a> /
        <a href="https://github.com/yxgeee/FD-GAN">code</a>
        <p></p>
      </td>
    </tr>
					
					
    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Website theme stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
          </p>
        </td>
      </tr>
    </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
